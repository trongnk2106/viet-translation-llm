<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Language Model-based Translator for English-Vietnamese Translation</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/skeleton/2.0.4/skeleton.min.css">
    <style>
        body {
            margin: 20px;
        }
        h1, h2, h3 {
            margin-top: 30px;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Advanced Language Model-based Translator for English-Vietnamese Translation</h1>

        <p>
            We introduce a transformative approach to English-Vietnamese translation, leveraging the cutting-edge capabilities of the Gemma-7B-IT (Gemma Team et al. 2024) model. Enhanced by the Advanced Language Model-based Translator (ALMA) (Xu et al. 2023) methodology, our system significantly advances beyond the conventional Transformer models in handling complex linguistic contexts.
        </p>

        <p>
            This research details our robust training framework, experimental validations, and the rigorous evaluation processes that establish a new state-of-the-art for Vietnamese translation tasks. Our results emphatically surpass those of well-known systems such as VinAI Translate (Nguyen et al. 2022) and Google Translate (Google 2024b), demonstrating an improvement of over 12 BLEU scores against the previously top-performing systems.
        </p>

        <p>
            These achievements highlight the superior flexibility and contextual understanding capabilities of Large Language Models (LLMs) (Zhao et al. 2023) integrated within our ALMA framework, which excel in adapting to varied translation nuances and complexities.
        </p>

        <p>
            Capitalizing on these remarkable advancements, we have also introduced a user-centric translation product, available at <a href="https://www.doctranslate.io" target="_blank">Doctranslate</a> (Doctranslate 2023). This tool embodies our commitment to merging technological innovation with practical utility, offering users a seamless and high-quality translation experience.
        </p>

        <h2>Getting Started</h2>

        <p>Follow the steps below to set up and run the project.</p>

        <h3>Clone the Repository</h3>

        <pre><code>
git clone &lt;repository_url&gt;
cd &lt;repository_directory&gt;
        </code></pre>

        <h3>Install requirements</h3>
        <p>Install the necessary packages using <code>requirements.txt</code>:</p>

        <pre><code>
pip install -r requirements.txt
        </code></pre>

        <h3>Custom Inference Configuration</h3>
        <p>Customize the inference configuration file <code>infer.yaml</code> according to your requirements. The <code>infer.yaml</code> file contains parameters for the translation model and other settings. Below is an example configuration:</p>

        <pre><code>
infer:
  modelname: "google/gemma-7b-it"           # The name of the model to use for translation
  adapter: "path adapter"                   # Path to the adapter, if any
  output_dir: "./translated.csv"            # Output file path for the translated text
  cache_dir: "./"                           # Directory for caching model files
  hf_tokens: "hf_tokens"                    # Hugging Face API tokens
  max_new_tokens: 512                       # Maximum number of tokens to generate
  num_beams: 5                              # Number of beams for beam search
  early_stopping: True                      # Whether to stop early when the model is confident
  no_repeat_ngram_size: 3                   # Size of n-grams that should not be repeated
  repetition_penalty: 1                     # Penalty for repeating tokens
  min_new_tokens: -1                        # Minimum number of tokens to generate
  length_penalty: 2.0                       # Penalty for the length of the generated text
  top_k: 50                                 # The number of highest probability vocabulary tokens to keep for top-k-filtering
  top_p: 0.95                               # Cumulative probability for top-p-filtering
  temperature: 0.1                          # Sampling temperature

translate:
  source_lang: "English"                    # Source language
  target_lang: "Vietnamese"                 # Target language
  file_path:                                # Path to the file (txt or csv) with the text to translate
  text: "Hi, I'm doc translate bot"         # Text to translate if no file is provided
        </code></pre>

        <h3>Running the Translation</h3>
        <p>Run the translation script with the customized configuration file:</p>

        <pre><code>
python utils/infer.py --config config/infer.yaml
        </code></pre>

        <h2>Repository Structure</h2>
        <p>The repository is organized as follows:</p>

        <ul>
            <li><code>config/</code>: Contains configuration files.</li>
            <li><code>utils/</code>: Contains utility scripts, including the inference script.</li>
            <li><code>requirements.txt</code>: Lists the required Python packages for the project.</li>
        </ul>

        <h2>License</h2>
        <p>This project is licensed under the MIT License. See the LICENSE file for details.</p>

        <h2>Acknowledgements</h2>
        <p>Special thanks to all contributors and the open-source community for their support and contributions.</p>
    </div>
</body>
</html>
